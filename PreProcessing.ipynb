{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "import pandas as pd\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f2f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count in range(1,5):\n",
    "    print(f'PRE-PROCESSING ON: StackSample{count}.csv')\n",
    "    stacksample = pd.read_csv(f'StackSample{count}.csv',sep=';')\n",
    "    display(stacksample.head())\n",
    "    \n",
    "    print(\"Removing HTML tags...\")\n",
    "    stacksample[\"Body\"] = stacksample[\"Body\"].progress_apply(lambda text: BeautifulSoup(text,'lxml').text)\n",
    "\n",
    "    print(\"Converting to lower case...\")\n",
    "    stacksample[\"Body\"] = stacksample[\"Body\"].str.lower()\n",
    "    stacksample[\"Title\"] = stacksample[\"Title\"].str.lower()\n",
    "    \n",
    "    print(\"Tokenizing using regular expressions...\")\n",
    "    pattern = r'''(?x)       # set flag to allow verbose regexps\n",
    "        \\w+[+#]+             # ending with pluses or hashes\n",
    "        | \\w+(?:[-.']+\\w+)*  # words with optional internal special characters\n",
    "        | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "        '''\n",
    "    stacksample[\"Tokenized Body\"] = stacksample[\"Body\"].progress_apply(lambda text: \\\n",
    "                                                                       nltk.regexp_tokenize(text, pattern))\n",
    "    stacksample[\"Tokenized Title\"] = stacksample[\"Title\"].progress_apply(lambda text: \\\n",
    "                                                                         nltk.regexp_tokenize(text, pattern))\n",
    "    \n",
    "    print(\"Removing useless stop words...\")\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    def filter_stopwords(words):\n",
    "        filtered_words = []\n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                filtered_words.append(word)\n",
    "        return filtered_words\n",
    "\n",
    "    stacksample[\"Tokenized Body\"] = stacksample[\"Tokenized Body\"].progress_apply(filter_stopwords)\n",
    "    stacksample[\"Tokenized Title\"] = stacksample[\"Tokenized Title\"].progress_apply(filter_stopwords)\n",
    "    display(stacksample.head())\n",
    "    \n",
    "    print(\"Converting to CSV format...\")\n",
    "    stacksample[[\"ID\",\"Tokenized Title\",\"Tokenized Body\",\"Tags\",\"Tag Count\"]].\\\n",
    "    to_csv(f\"StackSample{count}_Pre.csv\",sep=\";\", index=False)\n",
    "    pp = pd.read_csv(f\"StackSample{count}_Pre.csv\",sep=\";\")\n",
    "    display(pp.head())    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f75871",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c976d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('StackSample1_Pre.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokenized Title stem'] = df['Tokenized Title'].progress_apply(lambda x: [stemmer.stem(y) for y in ast.literal_eval(x)])\n",
    "df['Tokenized Body stem'] = df['Tokenized Body'].progress_apply(lambda x: [stemmer.stem(y) for y in ast.literal_eval(x)])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d6f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"ID\",\"Tokenized Title stem\",\"Tokenized Body stem\",\"Tags\",\"Tag Count\"]].to_csv(f\"StackSample1_Pre_stem.csv\",sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Tokenized Body lemm\"] = df[\"Tokenized Body\"].progress_apply(lambda x: [lemma.lemmatize(y, pos=\"v\") for y in ast.literal_eval(x)])\n",
    "df[\"Tokenized Title lemm\"] = df[\"Tokenized Title\"].progress_apply(lambda x: [lemma.lemmatize(y, pos=\"v\") for y in ast.literal_eval(x)])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"ID\",\"Tokenized Title lemm\",\"Tokenized Body lemm\",\"Tags\",\"Tag Count\"]].to_csv(f\"StackSample1_Pre_lemm.csv\",sep=\";\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
