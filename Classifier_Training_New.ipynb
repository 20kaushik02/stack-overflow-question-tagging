{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "db16930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import ast\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "import dill as pickled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "679e2107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../StackSample_kaggle.csv', encoding=\"utf-8\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13ab8e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ï»¿</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>good branch merge tutorials tortoisesvn</td>\n",
       "      <td>really good tutorials explain branch merge apa...</td>\n",
       "      <td>['svn']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>asp.net site map</td>\n",
       "      <td>anyone get experience create sql-based asp.net...</td>\n",
       "      <td>['sql', 'asp.net']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>function create color wheel</td>\n",
       "      <td>something pseudo-solved many time never quite ...</td>\n",
       "      <td>['algorithm']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>add script functionality .net applications</td>\n",
       "      <td>little game write c use database back-end trad...</td>\n",
       "      <td>['c#', '.net']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>use nest class case</td>\n",
       "      <td>work collection class use video playback recor...</td>\n",
       "      <td>['c++', 'oop', 'class']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ï»¿                                       Title  \\\n",
       "0           0    1     good branch merge tutorials tortoisesvn   \n",
       "1           1    2                            asp.net site map   \n",
       "2           2    3                 function create color wheel   \n",
       "3           3    4  add script functionality .net applications   \n",
       "4           4    5                         use nest class case   \n",
       "\n",
       "                                                Body                     Tags  \n",
       "0  really good tutorials explain branch merge apa...                  ['svn']  \n",
       "1  anyone get experience create sql-based asp.net...       ['sql', 'asp.net']  \n",
       "2  something pseudo-solved many time never quite ...            ['algorithm']  \n",
       "3  little game write c use database back-end trad...           ['c#', '.net']  \n",
       "4  work collection class use video playback recor...  ['c++', 'oop', 'class']  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b29f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Question'] = df['Title'] + df['Title'] + df['Title'] + df['Body'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f522c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        good branch merge tutorials tortoisesvngood br...\n",
       "1        asp.net site mapasp.net site mapasp.net site m...\n",
       "2        function create color wheelfunction create col...\n",
       "3        add script functionality .net applicationsadd ...\n",
       "4        use nest class caseuse nest class caseuse nest...\n",
       "                               ...                        \n",
       "58253    dramatic speed drop access static ram cache c+...\n",
       "58254    social framework watchkitsocial framework watc...\n",
       "58255    pass parameters template event meteorpass para...\n",
       "58256    msdtc server erver unavailablemsdtc server erv...\n",
       "58257    automatically implement classautomatically imp...\n",
       "Name: Question, Length: 58258, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "401da98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tags'] = df['Tags'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "887b4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'] = df['Tags'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a1ac2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  svn\n",
       "1          sql asp.net\n",
       "2            algorithm\n",
       "3              c# .net\n",
       "4        c++ oop class\n",
       "             ...      \n",
       "58253              c++\n",
       "58254      objective-c\n",
       "58255       javascript\n",
       "58256       sql-server\n",
       "58257          haskell\n",
       "Name: tags, Length: 58258, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a61766ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer = text_splitter, binary='true')\n",
    "multilabel_y = count_vectorizer.fit_transform(df['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aca47d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<58258x100 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 91838 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b4933ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_vectorizer.joblib']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(count_vectorizer, 'count_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4e5b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Question'], multilabel_y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "09f6e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitter(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "395fcbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", tokenizer = text_splitter, sublinear_tf=False, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cba3d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_multilabel = vectorizer.fit_transform(X_train.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4bcff994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.joblib']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer, 'vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4938ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_multilabel = vectorizer.transform(X_test.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f6b025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of train data X: (46606, 111133) Y : (46606, 100)\n",
      "Dimensions of test data X: (11652, 111133) Y: (11652, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d841c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_report(y_test, predictions):\n",
    "    print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "    print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "    precision = precision_score(y_test, predictions, average='micro')\n",
    "    recall = recall_score(y_test, predictions, average='micro')\n",
    "    f1 = f1_score(y_test, predictions, average='micro')\n",
    "\n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "    precision = precision_score(y_test, predictions, average='macro')\n",
    "    recall = recall_score(y_test, predictions, average='macro')\n",
    "    f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "    print(\"Macro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "    print (metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1aa0ae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.35264332303467216\n",
      "Hamming loss  0.009794026776519053\n",
      "Micro-average quality numbers\n",
      "Precision: 0.8016, Recall: 0.5039, F1-measure: 0.6188\n",
      "Macro-average quality numbers\n",
      "Precision: 0.7152, Recall: 0.4286, F1-measure: 0.5235\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.20      0.29       522\n",
      "           1       0.64      0.47      0.54        78\n",
      "           2       0.81      0.37      0.51       152\n",
      "           3       0.96      0.81      0.88       933\n",
      "           4       0.98      0.83      0.90       145\n",
      "           5       0.71      0.18      0.29        56\n",
      "           6       0.61      0.41      0.49       143\n",
      "           7       0.64      0.28      0.39       253\n",
      "           8       0.72      0.53      0.61       176\n",
      "           9       0.50      0.27      0.35        67\n",
      "          10       0.81      0.46      0.58        83\n",
      "          11       0.77      0.42      0.54       347\n",
      "          12       0.83      0.56      0.67      1348\n",
      "          13       0.89      0.65      0.75       782\n",
      "          14       0.79      0.44      0.57       113\n",
      "          15       0.17      0.02      0.03        59\n",
      "          16       0.50      0.11      0.18        72\n",
      "          17       0.12      0.01      0.02        87\n",
      "          18       0.79      0.65      0.71       341\n",
      "          19       0.64      0.10      0.18        68\n",
      "          20       0.45      0.14      0.21       111\n",
      "          21       0.57      0.29      0.39        41\n",
      "          22       0.56      0.24      0.34        83\n",
      "          23       0.93      0.66      0.77        77\n",
      "          24       0.74      0.23      0.35        61\n",
      "          25       0.97      0.77      0.85       145\n",
      "          26       0.75      0.59      0.66       155\n",
      "          27       0.97      0.78      0.86        49\n",
      "          28       0.76      0.60      0.67        75\n",
      "          29       0.45      0.18      0.26        55\n",
      "          30       0.80      0.61      0.69        66\n",
      "          31       0.17      0.03      0.05        38\n",
      "          32       0.70      0.25      0.36        57\n",
      "          33       0.87      0.43      0.57        77\n",
      "          34       0.93      0.83      0.88       192\n",
      "          35       0.86      0.63      0.73        57\n",
      "          36       0.61      0.52      0.56        54\n",
      "          37       0.98      0.68      0.80       128\n",
      "          38       0.93      0.59      0.72        64\n",
      "          39       0.58      0.33      0.42       412\n",
      "          40       0.80      0.44      0.57       111\n",
      "          41       0.43      0.18      0.25        50\n",
      "          42       0.35      0.14      0.20        57\n",
      "          43       0.72      0.47      0.57       445\n",
      "          44       0.82      0.27      0.41        52\n",
      "          45       0.67      0.34      0.45       330\n",
      "          46       0.86      0.60      0.71      1307\n",
      "          47       0.76      0.49      0.60       992\n",
      "          48       0.83      0.62      0.71       507\n",
      "          49       0.83      0.47      0.60       124\n",
      "          50       0.78      0.43      0.55        88\n",
      "          51       0.72      0.30      0.43       172\n",
      "          52       0.24      0.07      0.11        57\n",
      "          53       0.18      0.04      0.06        55\n",
      "          54       0.73      0.57      0.64        56\n",
      "          55       0.96      0.66      0.78        65\n",
      "          56       0.68      0.45      0.54       139\n",
      "          57       0.84      0.56      0.67       228\n",
      "          58       0.93      0.53      0.68       143\n",
      "          59       0.88      0.58      0.70        50\n",
      "          60       0.61      0.31      0.41       327\n",
      "          61       0.89      0.14      0.24        58\n",
      "          62       1.00      0.40      0.57        55\n",
      "          63       0.61      0.25      0.35       102\n",
      "          64       0.51      0.16      0.25       166\n",
      "          65       0.87      0.60      0.71        67\n",
      "          66       0.92      0.63      0.75       594\n",
      "          67       0.93      0.51      0.66        55\n",
      "          68       0.93      0.74      0.82       763\n",
      "          69       0.82      0.48      0.61        48\n",
      "          70       0.87      0.69      0.77       182\n",
      "          71       0.88      0.66      0.76       103\n",
      "          72       0.81      0.38      0.51        69\n",
      "          73       0.66      0.42      0.51       218\n",
      "          74       0.79      0.59      0.67       291\n",
      "          75       0.36      0.15      0.21        87\n",
      "          76       0.98      0.85      0.91       117\n",
      "          77       0.55      0.19      0.28        64\n",
      "          78       0.61      0.30      0.40        47\n",
      "          79       0.86      0.64      0.73        94\n",
      "          80       0.66      0.38      0.49       255\n",
      "          81       0.73      0.39      0.51       188\n",
      "          82       0.40      0.18      0.25        45\n",
      "          83       0.55      0.29      0.38       169\n",
      "          84       0.93      0.61      0.73        61\n",
      "          85       0.92      0.66      0.77        67\n",
      "          86       0.83      0.42      0.56        48\n",
      "          87       0.60      0.33      0.43        63\n",
      "          88       0.38      0.10      0.16        50\n",
      "          89       0.85      0.70      0.77        66\n",
      "          90       0.59      0.48      0.53        87\n",
      "          91       0.89      0.67      0.76        48\n",
      "          92       0.39      0.21      0.27       115\n",
      "          93       0.82      0.45      0.58        94\n",
      "          94       0.95      0.64      0.76        58\n",
      "          95       0.44      0.11      0.18       146\n",
      "          96       0.75      0.29      0.42        72\n",
      "          97       0.95      0.71      0.81       153\n",
      "          98       0.76      0.35      0.48       137\n",
      "          99       0.65      0.50      0.56       103\n",
      "\n",
      "   micro avg       0.80      0.50      0.62     18382\n",
      "   macro avg       0.72      0.43      0.52     18382\n",
      "weighted avg       0.77      0.50      0.60     18382\n",
      " samples avg       0.62      0.55      0.56     18382\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\CD_Project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "sgd_classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
    "sgd_classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = sgd_classifier.predict (x_test_multilabel)\n",
    "metrics_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce41a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.3631994507380707\n",
      "Hamming loss  0.009965671129419842\n",
      "Micro-average quality numbers\n",
      "Precision: 0.7605, Recall: 0.5375, F1-measure: 0.6299\n",
      "Macro-average quality numbers\n",
      "Precision: 0.6927, Recall: 0.4603, F1-measure: 0.5436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.24      0.30       522\n",
      "           1       0.63      0.47      0.54        78\n",
      "           2       0.79      0.47      0.59       152\n",
      "           3       0.95      0.85      0.90       933\n",
      "           4       0.98      0.88      0.93       145\n",
      "           5       0.67      0.18      0.28        56\n",
      "           6       0.64      0.44      0.52       143\n",
      "           7       0.60      0.34      0.43       253\n",
      "           8       0.70      0.55      0.62       176\n",
      "           9       0.64      0.27      0.38        67\n",
      "          10       0.82      0.51      0.63        83\n",
      "          11       0.74      0.49      0.59       347\n",
      "          12       0.74      0.57      0.65      1348\n",
      "          13       0.83      0.69      0.75       782\n",
      "          14       0.75      0.51      0.61       113\n",
      "          15       0.00      0.00      0.00        59\n",
      "          16       0.47      0.12      0.20        72\n",
      "          17       0.05      0.01      0.02        87\n",
      "          18       0.76      0.64      0.70       341\n",
      "          19       0.56      0.13      0.21        68\n",
      "          20       0.28      0.11      0.16       111\n",
      "          21       0.46      0.29      0.36        41\n",
      "          22       0.53      0.30      0.38        83\n",
      "          23       0.95      0.78      0.86        77\n",
      "          24       0.68      0.25      0.36        61\n",
      "          25       0.98      0.79      0.87       145\n",
      "          26       0.75      0.60      0.67       155\n",
      "          27       0.98      0.86      0.91        49\n",
      "          28       0.82      0.65      0.73        75\n",
      "          29       0.33      0.11      0.16        55\n",
      "          30       0.86      0.73      0.79        66\n",
      "          31       0.00      0.00      0.00        38\n",
      "          32       0.71      0.35      0.47        57\n",
      "          33       0.84      0.48      0.61        77\n",
      "          34       0.92      0.87      0.90       192\n",
      "          35       0.85      0.68      0.76        57\n",
      "          36       0.64      0.43      0.51        54\n",
      "          37       0.97      0.74      0.84       128\n",
      "          38       0.93      0.62      0.75        64\n",
      "          39       0.49      0.35      0.41       412\n",
      "          40       0.79      0.44      0.57       111\n",
      "          41       0.53      0.18      0.27        50\n",
      "          42       0.33      0.09      0.14        57\n",
      "          43       0.65      0.53      0.59       445\n",
      "          44       0.80      0.23      0.36        52\n",
      "          45       0.63      0.42      0.50       330\n",
      "          46       0.80      0.64      0.71      1307\n",
      "          47       0.71      0.53      0.61       992\n",
      "          48       0.82      0.65      0.73       507\n",
      "          49       0.79      0.49      0.61       124\n",
      "          50       0.81      0.53      0.64        88\n",
      "          51       0.61      0.34      0.44       172\n",
      "          52       0.26      0.09      0.13        57\n",
      "          53       0.33      0.07      0.12        55\n",
      "          54       0.75      0.64      0.69        56\n",
      "          55       0.96      0.74      0.83        65\n",
      "          56       0.63      0.43      0.51       139\n",
      "          57       0.82      0.58      0.68       228\n",
      "          58       0.94      0.62      0.74       143\n",
      "          59       0.92      0.66      0.77        50\n",
      "          60       0.56      0.35      0.43       327\n",
      "          61       0.41      0.16      0.23        58\n",
      "          62       0.92      0.42      0.57        55\n",
      "          63       0.57      0.29      0.39       102\n",
      "          64       0.41      0.18      0.25       166\n",
      "          65       0.90      0.64      0.75        67\n",
      "          66       0.91      0.67      0.77       594\n",
      "          67       0.92      0.60      0.73        55\n",
      "          68       0.90      0.75      0.82       763\n",
      "          69       0.85      0.46      0.59        48\n",
      "          70       0.93      0.82      0.87       182\n",
      "          71       0.82      0.73      0.77       103\n",
      "          72       0.83      0.42      0.56        69\n",
      "          73       0.61      0.42      0.50       218\n",
      "          74       0.80      0.63      0.70       291\n",
      "          75       0.40      0.18      0.25        87\n",
      "          76       0.95      0.84      0.89       117\n",
      "          77       0.66      0.30      0.41        64\n",
      "          78       0.45      0.28      0.34        47\n",
      "          79       0.90      0.67      0.77        94\n",
      "          80       0.53      0.37      0.44       255\n",
      "          81       0.70      0.43      0.53       188\n",
      "          82       0.33      0.13      0.19        45\n",
      "          83       0.53      0.33      0.40       169\n",
      "          84       0.95      0.66      0.78        61\n",
      "          85       0.92      0.72      0.81        67\n",
      "          86       0.89      0.52      0.66        48\n",
      "          87       0.61      0.37      0.46        63\n",
      "          88       0.27      0.08      0.12        50\n",
      "          89       0.85      0.68      0.76        66\n",
      "          90       0.64      0.51      0.56        87\n",
      "          91       0.94      0.67      0.78        48\n",
      "          92       0.48      0.26      0.34       115\n",
      "          93       0.82      0.44      0.57        94\n",
      "          94       0.95      0.69      0.80        58\n",
      "          95       0.56      0.16      0.25       146\n",
      "          96       0.73      0.33      0.46        72\n",
      "          97       0.92      0.73      0.81       153\n",
      "          98       0.65      0.39      0.49       137\n",
      "          99       0.70      0.52      0.60       103\n",
      "\n",
      "   micro avg       0.76      0.54      0.63     18382\n",
      "   macro avg       0.69      0.46      0.54     18382\n",
      "weighted avg       0.73      0.54      0.61     18382\n",
      " samples avg       0.65      0.59      0.59     18382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pa_classifier = OneVsRestClassifier(PassiveAggressiveClassifier(loss='log'), n_jobs=-1)\n",
    "pa_classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = pa_classifier.predict (x_test_multilabel)\n",
    "metrics_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c22e9fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.3713525575008582\n",
      "Hamming loss  0.009361483007209064\n",
      "Micro-average quality numbers\n",
      "Precision: 0.8524, Recall: 0.4917, F1-measure: 0.6237\n",
      "Macro-average quality numbers\n",
      "Precision: 0.7629, Recall: 0.4058, F1-measure: 0.5026\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.16      0.26       522\n",
      "           1       0.69      0.46      0.55        78\n",
      "           2       0.85      0.30      0.45       152\n",
      "           3       0.97      0.83      0.89       933\n",
      "           4       0.98      0.82      0.89       145\n",
      "           5       0.00      0.00      0.00        56\n",
      "           6       0.65      0.36      0.46       143\n",
      "           7       0.70      0.27      0.39       253\n",
      "           8       0.74      0.56      0.64       176\n",
      "           9       0.70      0.21      0.32        67\n",
      "          10       0.90      0.43      0.59        83\n",
      "          11       0.82      0.39      0.53       347\n",
      "          12       0.86      0.54      0.66      1348\n",
      "          13       0.90      0.64      0.75       782\n",
      "          14       0.83      0.39      0.53       113\n",
      "          15       0.00      0.00      0.00        59\n",
      "          16       0.78      0.10      0.17        72\n",
      "          17       0.00      0.00      0.00        87\n",
      "          18       0.83      0.65      0.73       341\n",
      "          19       1.00      0.04      0.08        68\n",
      "          20       1.00      0.01      0.02       111\n",
      "          21       0.69      0.22      0.33        41\n",
      "          22       0.60      0.22      0.32        83\n",
      "          23       0.96      0.68      0.79        77\n",
      "          24       0.83      0.16      0.27        61\n",
      "          25       0.98      0.76      0.86       145\n",
      "          26       0.77      0.61      0.68       155\n",
      "          27       1.00      0.82      0.90        49\n",
      "          28       0.84      0.63      0.72        75\n",
      "          29       0.58      0.13      0.21        55\n",
      "          30       0.86      0.74      0.80        66\n",
      "          31       0.00      0.00      0.00        38\n",
      "          32       0.87      0.23      0.36        57\n",
      "          33       0.85      0.45      0.59        77\n",
      "          34       0.95      0.88      0.91       192\n",
      "          35       0.90      0.63      0.74        57\n",
      "          36       0.71      0.46      0.56        54\n",
      "          37       0.99      0.68      0.81       128\n",
      "          38       0.97      0.59      0.74        64\n",
      "          39       0.64      0.27      0.38       412\n",
      "          40       0.92      0.42      0.58       111\n",
      "          41       0.67      0.08      0.14        50\n",
      "          42       0.00      0.00      0.00        57\n",
      "          43       0.72      0.52      0.61       445\n",
      "          44       0.83      0.19      0.31        52\n",
      "          45       0.77      0.31      0.44       330\n",
      "          46       0.87      0.61      0.72      1307\n",
      "          47       0.81      0.49      0.61       992\n",
      "          48       0.87      0.63      0.73       507\n",
      "          49       0.81      0.52      0.63       124\n",
      "          50       0.77      0.39      0.52        88\n",
      "          51       0.76      0.22      0.34       172\n",
      "          52       0.00      0.00      0.00        57\n",
      "          53       0.00      0.00      0.00        55\n",
      "          54       0.78      0.68      0.72        56\n",
      "          55       1.00      0.68      0.81        65\n",
      "          56       0.68      0.45      0.55       139\n",
      "          57       0.91      0.57      0.70       228\n",
      "          58       0.94      0.55      0.69       143\n",
      "          59       0.91      0.62      0.74        50\n",
      "          60       0.71      0.27      0.39       327\n",
      "          61       1.00      0.09      0.16        58\n",
      "          62       1.00      0.36      0.53        55\n",
      "          63       0.63      0.19      0.29       102\n",
      "          64       0.61      0.07      0.12       166\n",
      "          65       0.90      0.55      0.69        67\n",
      "          66       0.94      0.63      0.76       594\n",
      "          67       0.96      0.45      0.62        55\n",
      "          68       0.95      0.74      0.83       763\n",
      "          69       0.93      0.52      0.67        48\n",
      "          70       0.95      0.77      0.85       182\n",
      "          71       0.92      0.65      0.76       103\n",
      "          72       0.81      0.38      0.51        69\n",
      "          73       0.76      0.42      0.54       218\n",
      "          74       0.81      0.62      0.70       291\n",
      "          75       0.57      0.09      0.16        87\n",
      "          76       1.00      0.79      0.89       117\n",
      "          77       0.88      0.11      0.19        64\n",
      "          78       0.65      0.23      0.34        47\n",
      "          79       0.87      0.69      0.77        94\n",
      "          80       0.68      0.31      0.42       255\n",
      "          81       0.78      0.40      0.53       188\n",
      "          82       0.33      0.07      0.11        45\n",
      "          83       0.59      0.12      0.20       169\n",
      "          84       0.95      0.59      0.73        61\n",
      "          85       0.93      0.63      0.75        67\n",
      "          86       0.82      0.38      0.51        48\n",
      "          87       0.56      0.29      0.38        63\n",
      "          88       1.00      0.08      0.15        50\n",
      "          89       0.89      0.74      0.81        66\n",
      "          90       0.65      0.45      0.53        87\n",
      "          91       1.00      0.60      0.75        48\n",
      "          92       0.47      0.23      0.31       115\n",
      "          93       0.94      0.31      0.46        94\n",
      "          94       0.95      0.67      0.79        58\n",
      "          95       0.75      0.04      0.08       146\n",
      "          96       0.79      0.21      0.33        72\n",
      "          97       0.95      0.69      0.80       153\n",
      "          98       0.79      0.41      0.54       137\n",
      "          99       0.68      0.50      0.57       103\n",
      "\n",
      "   micro avg       0.85      0.49      0.62     18382\n",
      "   macro avg       0.76      0.41      0.50     18382\n",
      "weighted avg       0.81      0.49      0.59     18382\n",
      " samples avg       0.64      0.54      0.56     18382\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\CD_Project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\anaconda3\\envs\\CD_Project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\anaconda3\\envs\\CD_Project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "svc_classifier = OneVsRestClassifier(LinearSVC(penalty='l2', loss='hinge'), n_jobs=-1)\n",
    "svc_classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = svc_classifier.predict (x_test_multilabel)\n",
    "metrics_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "536cf202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.2482835564709921\n",
      "Hamming loss  0.011404050806728459\n",
      "Micro-average quality numbers\n",
      "Precision: 0.8492, Recall: 0.3370, F1-measure: 0.4825\n",
      "Macro-average quality numbers\n",
      "Precision: 0.7296, Recall: 0.2345, F1-measure: 0.3385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.12      0.20       522\n",
      "           1       0.70      0.29      0.41        78\n",
      "           2       0.90      0.17      0.29       152\n",
      "           3       0.98      0.68      0.80       933\n",
      "           4       0.98      0.44      0.61       145\n",
      "           5       0.00      0.00      0.00        56\n",
      "           6       0.67      0.31      0.42       143\n",
      "           7       0.69      0.18      0.28       253\n",
      "           8       0.73      0.38      0.50       176\n",
      "           9       0.60      0.04      0.08        67\n",
      "          10       0.95      0.24      0.38        83\n",
      "          11       0.87      0.25      0.39       347\n",
      "          12       0.86      0.41      0.56      1348\n",
      "          13       0.93      0.52      0.67       782\n",
      "          14       0.76      0.22      0.34       113\n",
      "          15       0.00      0.00      0.00        59\n",
      "          16       0.00      0.00      0.00        72\n",
      "          17       0.00      0.00      0.00        87\n",
      "          18       0.84      0.53      0.65       341\n",
      "          19       0.00      0.00      0.00        68\n",
      "          20       0.25      0.03      0.05       111\n",
      "          21       0.67      0.15      0.24        41\n",
      "          22       0.83      0.12      0.21        83\n",
      "          23       1.00      0.26      0.41        77\n",
      "          24       1.00      0.05      0.09        61\n",
      "          25       1.00      0.48      0.64       145\n",
      "          26       0.82      0.37      0.51       155\n",
      "          27       1.00      0.37      0.54        49\n",
      "          28       0.73      0.21      0.33        75\n",
      "          29       0.40      0.07      0.12        55\n",
      "          30       0.86      0.36      0.51        66\n",
      "          31       0.00      0.00      0.00        38\n",
      "          32       1.00      0.11      0.19        57\n",
      "          33       0.88      0.18      0.30        77\n",
      "          34       0.96      0.74      0.84       192\n",
      "          35       0.94      0.30      0.45        57\n",
      "          36       0.78      0.26      0.39        54\n",
      "          37       0.98      0.31      0.47       128\n",
      "          38       1.00      0.16      0.27        64\n",
      "          39       0.65      0.23      0.34       412\n",
      "          40       0.92      0.21      0.34       111\n",
      "          41       0.60      0.06      0.11        50\n",
      "          42       0.40      0.04      0.06        57\n",
      "          43       0.77      0.32      0.45       445\n",
      "          44       1.00      0.10      0.18        52\n",
      "          45       0.71      0.17      0.28       330\n",
      "          46       0.87      0.43      0.57      1307\n",
      "          47       0.79      0.39      0.52       992\n",
      "          48       0.86      0.48      0.61       507\n",
      "          49       0.82      0.30      0.44       124\n",
      "          50       0.88      0.26      0.40        88\n",
      "          51       0.79      0.13      0.23       172\n",
      "          52       0.25      0.02      0.03        57\n",
      "          53       0.00      0.00      0.00        55\n",
      "          54       0.77      0.43      0.55        56\n",
      "          55       1.00      0.25      0.40        65\n",
      "          56       0.71      0.30      0.42       139\n",
      "          57       0.90      0.38      0.54       228\n",
      "          58       0.97      0.20      0.34       143\n",
      "          59       0.92      0.24      0.38        50\n",
      "          60       0.63      0.17      0.27       327\n",
      "          61       0.00      0.00      0.00        58\n",
      "          62       1.00      0.18      0.31        55\n",
      "          63       0.75      0.12      0.20       102\n",
      "          64       0.64      0.05      0.10       166\n",
      "          65       0.89      0.25      0.40        67\n",
      "          66       0.95      0.42      0.58       594\n",
      "          67       1.00      0.15      0.25        55\n",
      "          68       0.95      0.53      0.68       763\n",
      "          69       0.89      0.17      0.28        48\n",
      "          70       0.96      0.49      0.65       182\n",
      "          71       0.98      0.40      0.57       103\n",
      "          72       1.00      0.12      0.21        69\n",
      "          73       0.65      0.25      0.36       218\n",
      "          74       0.82      0.43      0.56       291\n",
      "          75       0.54      0.08      0.14        87\n",
      "          76       1.00      0.44      0.62       117\n",
      "          77       0.50      0.02      0.03        64\n",
      "          78       0.67      0.13      0.21        47\n",
      "          79       0.91      0.41      0.57        94\n",
      "          80       0.66      0.28      0.40       255\n",
      "          81       0.77      0.32      0.45       188\n",
      "          82       0.00      0.00      0.00        45\n",
      "          83       0.52      0.18      0.26       169\n",
      "          84       0.91      0.34      0.50        61\n",
      "          85       1.00      0.25      0.40        67\n",
      "          86       1.00      0.08      0.15        48\n",
      "          87       0.65      0.17      0.28        63\n",
      "          88       0.00      0.00      0.00        50\n",
      "          89       0.93      0.38      0.54        66\n",
      "          90       0.64      0.37      0.47        87\n",
      "          91       1.00      0.33      0.50        48\n",
      "          92       0.45      0.18      0.26       115\n",
      "          93       1.00      0.23      0.38        94\n",
      "          94       1.00      0.26      0.41        58\n",
      "          95       0.47      0.05      0.10       146\n",
      "          96       1.00      0.04      0.08        72\n",
      "          97       0.95      0.39      0.55       153\n",
      "          98       0.75      0.22      0.34       137\n",
      "          99       0.75      0.32      0.45       103\n",
      "\n",
      "   micro avg       0.85      0.34      0.48     18382\n",
      "   macro avg       0.73      0.23      0.34     18382\n",
      "weighted avg       0.80      0.34      0.46     18382\n",
      " samples avg       0.45      0.37      0.39     18382\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\CD_Project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\anaconda3\\envs\\CD_Project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\anaconda3\\envs\\CD_Project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "logreg_classifier = OneVsRestClassifier(LogisticRegression(penalty='l2', max_iter=500, n_jobs=-1))\n",
    "logreg_classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = logreg_classifier.predict (x_test_multilabel)\n",
    "metrics_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "399a4992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.20537246824579472\n",
      "Hamming loss  0.015929454170957775\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4956, Recall: 0.5536, F1-measure: 0.5230\n",
      "Macro-average quality numbers\n",
      "Precision: 0.4969, Recall: 0.4915, F1-measure: 0.4655\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.32      0.27       522\n",
      "           1       0.39      0.56      0.46        78\n",
      "           2       0.50      0.41      0.45       152\n",
      "           3       0.83      0.88      0.85       933\n",
      "           4       0.90      0.86      0.88       145\n",
      "           5       0.13      0.05      0.08        56\n",
      "           6       0.32      0.08      0.13       143\n",
      "           7       0.35      0.38      0.36       253\n",
      "           8       0.46      0.57      0.51       176\n",
      "           9       0.38      0.37      0.38        67\n",
      "          10       0.69      0.49      0.58        83\n",
      "          11       0.49      0.17      0.26       347\n",
      "          12       0.48      0.70      0.57      1348\n",
      "          13       0.66      0.73      0.69       782\n",
      "          14       0.47      0.69      0.56       113\n",
      "          15       0.15      0.31      0.20        59\n",
      "          16       0.34      0.28      0.31        72\n",
      "          17       0.12      0.05      0.07        87\n",
      "          18       0.60      0.60      0.60       341\n",
      "          19       0.23      0.43      0.30        68\n",
      "          20       0.09      0.51      0.16       111\n",
      "          21       0.38      0.49      0.43        41\n",
      "          22       0.28      0.10      0.14        83\n",
      "          23       0.89      0.82      0.85        77\n",
      "          24       0.37      0.33      0.35        61\n",
      "          25       0.96      0.51      0.67       145\n",
      "          26       0.66      0.61      0.63       155\n",
      "          27       0.95      0.78      0.85        49\n",
      "          28       0.71      0.68      0.69        75\n",
      "          29       0.45      0.16      0.24        55\n",
      "          30       0.64      0.77      0.70        66\n",
      "          31       0.11      0.08      0.09        38\n",
      "          32       0.54      0.33      0.41        57\n",
      "          33       0.41      0.58      0.48        77\n",
      "          34       0.92      0.71      0.80       192\n",
      "          35       0.66      0.47      0.55        57\n",
      "          36       0.20      0.02      0.03        54\n",
      "          37       0.92      0.51      0.65       128\n",
      "          38       0.88      0.58      0.70        64\n",
      "          39       0.40      0.27      0.32       412\n",
      "          40       0.42      0.57      0.48       111\n",
      "          41       0.16      0.06      0.09        50\n",
      "          42       0.25      0.05      0.09        57\n",
      "          43       0.63      0.50      0.56       445\n",
      "          44       0.75      0.35      0.47        52\n",
      "          45       0.24      0.64      0.35       330\n",
      "          46       0.80      0.44      0.57      1307\n",
      "          47       0.64      0.49      0.55       992\n",
      "          48       0.62      0.68      0.65       507\n",
      "          49       0.64      0.76      0.70       124\n",
      "          50       0.48      0.66      0.56        88\n",
      "          51       0.25      0.52      0.34       172\n",
      "          52       0.00      0.00      0.00        57\n",
      "          53       0.00      0.00      0.00        55\n",
      "          54       0.21      0.88      0.34        56\n",
      "          55       0.88      0.65      0.74        65\n",
      "          56       0.47      0.79      0.59       139\n",
      "          57       0.51      0.68      0.58       228\n",
      "          58       0.71      0.55      0.62       143\n",
      "          59       0.79      0.62      0.70        50\n",
      "          60       0.60      0.42      0.49       327\n",
      "          61       0.32      0.17      0.22        58\n",
      "          62       0.66      0.56      0.61        55\n",
      "          63       0.39      0.15      0.21       102\n",
      "          64       0.16      0.25      0.19       166\n",
      "          65       0.55      0.78      0.65        67\n",
      "          66       0.75      0.74      0.74       594\n",
      "          67       0.20      0.65      0.31        55\n",
      "          68       0.57      0.76      0.65       763\n",
      "          69       0.77      0.56      0.65        48\n",
      "          70       0.44      0.86      0.58       182\n",
      "          71       0.49      0.69      0.57       103\n",
      "          72       0.74      0.41      0.52        69\n",
      "          73       0.57      0.50      0.54       218\n",
      "          74       0.55      0.73      0.63       291\n",
      "          75       0.15      0.57      0.24        87\n",
      "          76       0.67      0.85      0.75       117\n",
      "          77       0.19      0.45      0.27        64\n",
      "          78       0.12      0.02      0.04        47\n",
      "          79       0.58      0.74      0.65        94\n",
      "          80       0.48      0.21      0.29       255\n",
      "          81       0.45      0.48      0.47       188\n",
      "          82       0.17      0.47      0.25        45\n",
      "          83       0.20      0.75      0.31       169\n",
      "          84       0.81      0.64      0.72        61\n",
      "          85       0.88      0.69      0.77        67\n",
      "          86       0.81      0.60      0.69        48\n",
      "          87       0.47      0.40      0.43        63\n",
      "          88       0.27      0.12      0.17        50\n",
      "          89       0.85      0.61      0.71        66\n",
      "          90       0.53      0.47      0.50        87\n",
      "          91       0.80      0.42      0.55        48\n",
      "          92       0.21      0.17      0.18       115\n",
      "          93       0.39      0.61      0.47        94\n",
      "          94       0.78      0.72      0.75        58\n",
      "          95       0.18      0.29      0.22       146\n",
      "          96       0.41      0.64      0.50        72\n",
      "          97       0.91      0.76      0.83       153\n",
      "          98       0.68      0.47      0.55       137\n",
      "          99       0.31      0.65      0.42       103\n",
      "\n",
      "   micro avg       0.50      0.55      0.52     18382\n",
      "   macro avg       0.50      0.49      0.47     18382\n",
      "weighted avg       0.56      0.55      0.53     18382\n",
      " samples avg       0.51      0.60      0.51     18382\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\CD_Project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "perc_classifier = OneVsRestClassifier(Perceptron(alpha=0.00001, penalty='l1', n_jobs=-1))\n",
    "perc_classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = perc_classifier.predict (x_test_multilabel)\n",
    "metrics_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ef8bdc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Perceptron_2_clf.joblib']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(sgd_classifier, 'SGD_2_clf.joblib')\n",
    "joblib.dump(logreg_classifier, 'LogReg_2_clf.joblib')\n",
    "joblib.dump(pa_classifier, 'PassAgg_2_clf.joblib')\n",
    "joblib.dump(svc_classifier, 'LinearSVC_2_clf.joblib')\n",
    "joblib.dump(perc_classifier,'Perceptron_2_clf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ee4cdec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_2 = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "94d8db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_multilabel = vectorizer_2.fit_transform(X_train.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5d241786",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vec = open(\"test_vec.pkl\", 'wb')\n",
    "pickled.dump(vectorizer_2, save_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "be93bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_count_vec = open(\"test_count_vec.pickle\", 'wb')\n",
    "pickled.dump(count_vectorizer, save_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "59b9d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_count_vec.pickle\", \"wb\") as f:\n",
    "    pickled.dump(count_vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ae84c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_vec.pickle\", \"wb\") as f:\n",
    "    pickled.dump(vectorizer_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48b723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
